model:
  _target_: model.BaseModel
  encoder:
    _target_: networks.TransformerEncoderModel
    resblock_config:
    - in_filters: 1
      out_filters: 16
      dilation: 1
      is_resid: true
    - in_filters: 16
      out_filters: 16
      dilation: 2
    - in_filters: 16
      out_filters: 32
      dilation: 4
    - in_filters: 32
      out_filters: 32
      dilation: 8
    - in_filters: 32
      out_filters: 32
      dilation: 16
    nhead: 8
    num_attention_layers: 4
    dim_feedforward: 128
    dropout: 0.1
    return_valid: true
  decoder:
    _target_: networks.LinearDecoder
    input_dim: 128
    intermediate_dim: 128
    num_classes: 3
  loss_fn:
    _target_: losses.CEtransitionLoss
    smoothness_weight: 0.2
    transition_penalty_weight: 1
    label_smoothing: 0.05
  learning_rate: 0.001
  weight_decay: 1.0e-05
  return_valid: false
  lr_scheduler_config:
    step_size: 5
    gamma: 0.75
datamodule:
  _target_: dataset.WindowedEEGDataModule
  window_length: 150
  batch_size: 256
  num_workers: 6
  train_dir: data/processed/train
  val_dir: data/processed/val
  test_dir: data/processed/test
  stride: 50
  keep_ratio: 1.5
  apply_savgol: 0
  deriv: 0
  transforms:
  - _target_: utils.transforms.population_zscore_transform
trainer:
  min_epochs: 15
  max_epochs: 50
  log_every_n_steps: 1
loss:
  smoothness_weight: 0.2
  transition_penalty_weight: 0
  label_smoothing: 0.05
name: SOD_trans-lin_comp-attention_lay
y
